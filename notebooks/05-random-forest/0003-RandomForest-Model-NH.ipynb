{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Random Forest Models</center></h1>\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Bag-of-Words Vectoriser\n",
    "## Standard Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW DATA - BoW, Metrics:\n",
      "\n",
      "Logistic Regression Accuracy:\n",
      " 0.700339558573854 \n",
      "\n",
      "Logistic Regression Precision:\n",
      " 0.707890521479854 \n",
      "\n",
      "Logistic Regression Recall:\n",
      " 0.6974185379443766 \n",
      "\n",
      "Logistic Regression F1 Score:\n",
      " 0.7014634744052103\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import string\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Reading .csv file\n",
    "\n",
    "df_train = pd.read_csv(r\"C:\\Users\\nathi_000\\Desktop\\Python Files\\NLP Project\\nlp-web-scrapping\\data\\raw\\tweets-train.csv\")\n",
    "\n",
    "df_test = pd.read_csv(r\"C:\\Users\\nathi_000\\Desktop\\Python Files\\NLP Project\\nlp-web-scrapping\\data\\raw\\tweets-test.csv\")\n",
    "\n",
    "X_train = df_train['text'].astype(str)\n",
    "Y_train = df_train['sentiment'].astype(str)\n",
    "\n",
    "X_test = df_test['text'].astype(str)\n",
    "Y_test = df_test['sentiment'].astype(str)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating custom tokeniser and cleaning function\n",
    "\n",
    "Lemmatiser = nltk.stem.WordNetLemmatizer()\n",
    "# Instantiating the NLTK Lemmatiser\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Putting punctuation symbols into an object\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Import spacy model\n",
    "\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# A list of stopwords that can be filtered out\n",
    "    # NLTK also has a stop words object but it has fewer words\n",
    "\n",
    "def text_cleaner(sentence):    \n",
    "                \n",
    "    sentence = \"\".join([char for char in sentence.strip() if char not in punctuations])\n",
    "    # Getting rid of any punctuation characters\n",
    "    \n",
    "    myTokens = re.split('\\W+', sentence)\n",
    "    # Tokenising the words\n",
    "    \n",
    "    myTokens = [token.lower() for token in myTokens if token not in stopwords]\n",
    "    # Removing stop words\n",
    "    \n",
    "    myTokens = [Lemmatiser.lemmatize(token) for token in myTokens]\n",
    "    # Lemmatising the words and putting in lower case except for proper nouns\n",
    "    \n",
    "    return myTokens    \n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating Vectoriser and Classifiers\n",
    "\n",
    "bow_vector = CountVectorizer(tokenizer = text_cleaner, ngram_range=(1,1))\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "pipe = Pipeline([('vectorizer', bow_vector)\n",
    "                 ,('classifier', rfc)])\n",
    "\n",
    "pipe.fit(X_train, Y_train)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"RAW DATA - BoW, Metrics:\\n\")\n",
    "print(\"Logistic Regression Accuracy:\\n\",metrics.accuracy_score(Y_test, predicted),\"\\n\") # Accuracy\n",
    "print(\"Logistic Regression Precision:\\n\",metrics.precision_score(Y_test, predicted, average='macro'),\"\\n\") # Precision\n",
    "print(\"Logistic Regression Recall:\\n\",metrics.recall_score(Y_test, predicted, average='macro'),\"\\n\") # Recall\n",
    "print(\"Logistic Regression F1 Score:\\n\",metrics.f1_score(Y_test, predicted, average='macro')) # F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Bag-of-Words Vectoriser\n",
    "## K-Fold Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW DATA - BoW, K-Fold:\n",
      "\n",
      "[0.69092232 0.69796215 0.694869   0.69432314 0.70178311]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import string\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Reading .csv file\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\nathi_000\\Desktop\\Python Files\\NLP Project\\nlp-web-scrapping\\data\\raw\\tweets-train.csv\")\n",
    "\n",
    "X = df['text'].astype(str)\n",
    "# Convert to type 'string' as pandas converts inputs to their most relevant type\n",
    "    # The issue is sometimes pandas converts data to a 'float'; this doesn't work with the evaluation functions\n",
    "\n",
    "Y = df['sentiment'].astype(str)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating custom tokeniser and cleaning function\n",
    "\n",
    "Lemmatiser = nltk.stem.WordNetLemmatizer()\n",
    "# Instantiating the NLTK Lemmatiser\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Putting punctuation symbols into an object\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Import spacy model\n",
    "\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# A list of stopwords that can be filtered out\n",
    "    # NLTK also has a stop words object but it has fewer words\n",
    "\n",
    "def text_cleaner(sentence):    \n",
    "                \n",
    "    sentence = \"\".join([char for char in sentence.strip() if char not in punctuations])\n",
    "    # Getting rid of any punctuation characters\n",
    "    \n",
    "    myTokens = re.split('\\W+', sentence)\n",
    "    # Tokenising the words\n",
    "    \n",
    "    myTokens = [token.lower() for token in myTokens if token not in stopwords]\n",
    "    # Removing stop words\n",
    "    \n",
    "    myTokens = [Lemmatiser.lemmatize(token) for token in myTokens]\n",
    "    # Lemmatising the words and putting in lower case except for proper nouns\n",
    "    \n",
    "    return myTokens    \n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating Vectoriser and Classifiers\n",
    "\n",
    "bow_vector = CountVectorizer(tokenizer = text_cleaner, ngram_range=(1,1))\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "pipe = Pipeline([('vectorizer', bow_vector)\n",
    "                 ,('classifier', rfc)])\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "# 5 splits\n",
    "\n",
    "print(\"RAW DATA - BoW, K-Fold:\\n\")\n",
    "print(cross_val_score(estimator=pipe, X=X, y=Y, cv=k_fold, scoring='accuracy', n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# TF-IDF Vectoriser\n",
    "## Standard Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW DATA - TF-IDF, Metrics:\n",
      "\n",
      "Logistic Regression Accuracy:\n",
      " 0.7051499717034522 \n",
      "\n",
      "Logistic Regression Precision:\n",
      " 0.714771804687231 \n",
      "\n",
      "Logistic Regression Recall:\n",
      " 0.7010965764365583 \n",
      "\n",
      "Logistic Regression F1 Score:\n",
      " 0.706177608947038\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import string\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Reading .csv file\n",
    "\n",
    "df_train = pd.read_csv(r\"C:\\Users\\nathi_000\\Desktop\\Python Files\\NLP Project\\nlp-web-scrapping\\data\\raw\\tweets-train.csv\")\n",
    "\n",
    "df_test = pd.read_csv(r\"C:\\Users\\nathi_000\\Desktop\\Python Files\\NLP Project\\nlp-web-scrapping\\data\\raw\\tweets-test.csv\")\n",
    "\n",
    "X_train = df_train['text'].astype(str)\n",
    "Y_train = df_train['sentiment'].astype(str)\n",
    "\n",
    "X_test = df_test['text'].astype(str)\n",
    "Y_test = df_test['sentiment'].astype(str)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating custom tokeniser and cleaning function\n",
    "\n",
    "Lemmatiser = nltk.stem.WordNetLemmatizer()\n",
    "# Instantiating the NLTK Lemmatiser\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Putting punctuation symbols into an object\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Import spacy model\n",
    "\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# A list of stopwords that can be filtered out\n",
    "    # NLTK also has a stop words object but it has fewer words\n",
    "\n",
    "def text_cleaner(sentence):    \n",
    "                \n",
    "    sentence = \"\".join([char for char in sentence.strip() if char not in punctuations])\n",
    "    # Getting rid of any punctuation characters\n",
    "    \n",
    "    myTokens = re.split('\\W+', sentence)\n",
    "    # Tokenising the words\n",
    "    \n",
    "    myTokens = [token.lower() for token in myTokens if token not in stopwords]\n",
    "    # Removing stop words\n",
    "    \n",
    "    myTokens = [Lemmatiser.lemmatize(token) for token in myTokens]\n",
    "    # Lemmatising the words and putting in lower case except for proper nouns\n",
    "    \n",
    "    return myTokens    \n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating Vectoriser and Classifiers\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = text_cleaner)\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "pipe = Pipeline([('vectorizer', tfidf_vector)\n",
    "                 ,('classifier', rfc)])\n",
    "\n",
    "pipe.fit(X_train, Y_train)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"RAW DATA - TF-IDF, Metrics:\\n\")\n",
    "print(\"Logistic Regression Accuracy:\\n\",metrics.accuracy_score(Y_test, predicted),\"\\n\") # Accuracy\n",
    "print(\"Logistic Regression Precision:\\n\",metrics.precision_score(Y_test, predicted, average='macro'),\"\\n\") # Precision\n",
    "print(\"Logistic Regression Recall:\\n\",metrics.recall_score(Y_test, predicted, average='macro'),\"\\n\") # Recall\n",
    "print(\"Logistic Regression F1 Score:\\n\",metrics.f1_score(Y_test, predicted, average='macro')) # F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# TF-IDF Vectoriser\n",
    "## K-Fold Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW DATA - TF-IDF, K-Fold:\n",
      "\n",
      "[0.69474259 0.70342067 0.70742358 0.70160116 0.70178311]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import string\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Reading .csv file\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\nathi_000\\Desktop\\Python Files\\NLP Project\\nlp-web-scrapping\\data\\raw\\tweets-train.csv\")\n",
    "\n",
    "X = df['text'].astype(str)\n",
    "# Convert to type 'string' as pandas converts inputs to their most relevant type\n",
    "    # The issue is sometimes pandas converts data to a 'float'; this doesn't work with the evaluation functions\n",
    "\n",
    "Y = df['sentiment'].astype(str)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating custom tokeniser and cleaning function\n",
    "\n",
    "Lemmatiser = nltk.stem.WordNetLemmatizer()\n",
    "# Instantiating the NLTK Lemmatiser\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Putting punctuation symbols into an object\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Import spacy model\n",
    "\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# A list of stopwords that can be filtered out\n",
    "    # NLTK also has a stop words object but it has fewer words\n",
    "\n",
    "def text_cleaner(sentence):    \n",
    "                \n",
    "    sentence = \"\".join([char for char in sentence.strip() if char not in punctuations])\n",
    "    # Getting rid of any punctuation characters\n",
    "    \n",
    "    myTokens = re.split('\\W+', sentence)\n",
    "    # Tokenising the words\n",
    "    \n",
    "    myTokens = [token.lower() for token in myTokens if token not in stopwords]\n",
    "    # Removing stop words\n",
    "    \n",
    "    myTokens = [Lemmatiser.lemmatize(token) for token in myTokens]\n",
    "    # Lemmatising the words and putting in lower case except for proper nouns\n",
    "    \n",
    "    return myTokens    \n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating Vectoriser\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = text_cleaner)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "    \n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "pipe = Pipeline([('vectorizer', tfidf_vector)\n",
    "                 ,('classifier', rfc)])\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "# 5 splits\n",
    "\n",
    "print(\"RAW DATA - TF-IDF, K-Fold:\\n\")\n",
    "print(cross_val_score(estimator=pipe, X=X, y=Y, cv=k_fold, scoring='accuracy', n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
