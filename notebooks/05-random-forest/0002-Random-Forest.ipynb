{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Random Forest Classification </center></h1>\n",
    "\n",
    "LinkedIn Learning: https://www.linkedin.com/learning/nlp-with-python-for-machine-learning-essential-training/introducing-random-forest?u=78163626\n",
    "\n",
    "## *Written by Nathanael Hitch*\n",
    "\n",
    "<span style=\"color:OrangeRed\">For background in classification - 'NLP_Logisitic-Regression.ipynb'</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before - Ensemble Method\n",
    "\n",
    "**Ensemble method** is a technique that \"creates multiple models and combines them to produce better results than any single model\"/<br>\n",
    "This helps the model to depend on aggregated opinions of many models rather than the individual opinion of one model.\n",
    "\n",
    "# What is it?\n",
    "\n",
    "**Randon Forest** is an \"Ensemble learning method that constructs a collection of decision trees and then aggregates the predictions of each tree to determine the final prediction\".<br>\n",
    "The individual decision trees are the 'weak models' that are combined into the aggreagated Forest model.\n",
    "\n",
    "In a binary classification (positive or negative), each individually built decision tree will give a result of positive or negative with the aggregated result being the classification with the most results (positive = 60, negative = 40, positive is the aggregate result).<br>\n",
    "Simply put, it's a simple voting method for the trees.\n",
    "\n",
    "## Benefits\n",
    "\n",
    "- Can be used for classification (categorical) or regression (continous).\n",
    "- Easily handles outliers, missing values, scewed data or data that's not on the same scale.\n",
    "- Accepts various types of inputs (continous, ordinal/statistical, etc.).\n",
    "- Less likely to Overfit.\n",
    "- Outputs feature importance.\n",
    "\n",
    "Useful as data cleaning is rarely needed as it accepts anything.\n",
    "\n",
    "# Example\n",
    "\n",
    "## Set Up\n",
    "\n",
    "This starts very similar to Logistic Regression:\n",
    "\n",
    "- read in file\n",
    "- tokeniser and clean the data<br>\n",
    "In this case, tokenise, remove stop words and punctuation and lemmatise.<br>\n",
    "- create vectoriser: Count or Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        # Importing needed packages\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import string\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Reading .csv file\n",
    "\n",
    "#df = pd.read_csv(\"Files/SMSSpamCollection.tsv\", sep='\\t')\n",
    "\n",
    "df = pd.read_csv(\"Files/SMS_test.tsv\", sep='\\t')\n",
    "# Smaller testing file\n",
    "\n",
    "df.columns = ['label','body_test']\n",
    "# File doesn't have column headers - now they have names assigned to their columns\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating custom tokeniser and cleaning function\n",
    "\n",
    "def spacy_cleaner(sentence):\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    punctuations = string.punctuation\n",
    "    \n",
    "    #print(\"Input sentence:\\n\", sentence,\"\\n\")\n",
    "    \n",
    "    doc = nlp(sentence.strip())\n",
    "    # Pass text into model's pipeline.\n",
    "    \n",
    "    myTokens = [token for token in doc]\n",
    "    # Creating a list of the words in the sentence.\n",
    "    #print(\"Sentence tokenised:\\n\", myTokens,\"\\n\")\n",
    "    \n",
    "    myTokens = [token for token in myTokens if token.is_stop == False and token.text not in punctuations]\n",
    "    # List of words without stopwords or punctuations.\n",
    "    #print(\"Sentence without stopwords or punctuations:\\n\", myTokens, \"\\n\")\n",
    "    \n",
    "    myTokens = [token.lemma_.strip().lower() if token.pos_ != \"PROPN\" else token.lemma_.strip() \\\n",
    "                for token in myTokens]\n",
    "    # Words are lemmatised, spaces at end removed and (if not a proper noun) lowercased.\n",
    "    \n",
    "    myTokens = [token for token in myTokens if token != \"\"]\n",
    "    \n",
    "    #print(\"Sentence lemmatisted, no spaces and lowercase (except Proper Noun):\\n\", myTokens, \"\\n\")\n",
    "    \n",
    "    return myTokens\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating Vectoriser\n",
    "\n",
    "bow_vector = CountVectorizer(tokenizer = spacy_cleaner, ngram_range=(1,1))\n",
    "\n",
    "#tfidf_vector = TfidfVectorizer(tokenizer = spacy_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Random Forest Classifier\n",
    "\n",
    "Next we need to create the Random Forest object; to see the list of all the attributes and methods for the Random Forest Classifier, use *dir(RandomForestClassifier)*.<br>\n",
    "By printing *RandomForestClassifier()* it will also show all the hyper-primaries plus their default settings.\n",
    "\n",
    "This object has n_jobs = -1 as it'll allow the decision trees to be made independent of one another, in parallel, so that they can be made faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_estimator_type', '_get_param_names', '_get_tags', '_make_estimator', '_more_tags', '_required_parameters', '_set_oob_score', '_validate_X_predict', '_validate_estimator', '_validate_y_class_weight', 'apply', 'base_estimator', 'bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'decision_path', 'estimator_params', 'feature_importances_', 'fit', 'get_params', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'predict', 'predict_log_proba', 'predict_proba', 'random_state', 'score', 'set_params', 'verbose', 'warm_start'] \n",
      "\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=4, n_jobs=-1,\n",
      "                       oob_score=False, random_state=None, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1, n_estimators=4)\n",
    "# n_jobs allows the decision trees to be made in parallel\n",
    "    # The model is trained and tested more quickly.\n",
    "\n",
    "print(dir(rfc),\"\\n\")\n",
    "# Lists the classes Attributes and Methods\n",
    "\n",
    "print(rfc)\n",
    "# Shows the Hyper-primaries and their default values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amongst a lot of hyper-primaries, *I will not be going through every one*, there is **n_estimators**. This is the number of decision trees that will be buld within the model; at the moment there will be '10' trees built with unlimited steps. There will then be a vote amongst the trees to make the final decision.\n",
    "\n",
    "Also of significance is **max_depth** which is \"*how deep each of the decision trees is*\". It's default is 'None' which means that the model build each decision tree until it minimises some loss criteria.<br>\n",
    "<span style=\"background-color:red; color:white\">I'll be honest, I'm not completely sure what this means.</span>\n",
    "\n",
    "*As a note*, Random Forest is built on relatively few fully built decision trees; this will become apparent later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "\n",
    "### Standard\n",
    "\n",
    "One way is to split the data into training and testing groups and train and test the model that way.\n",
    "\n",
    "<span style=\"background-color:red; color:white\">NOTE:</span>\n",
    "`For this way, the data has been reduced and the number of decision trees has been lessened, otherwise it takes the computer a long time to process the information.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy:\n",
      " 0.9066666666666666 \n",
      "\n",
      "Random Forest Precision:\n",
      " 0.9507042253521127 \n",
      "\n",
      "Random Forest Recall:\n",
      " 0.6818181818181819 \n",
      "\n",
      "Random Forest F1 Score:\n",
      " 0.7407407407407407\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Splitting the test and training\n",
    "    \n",
    "X = df['body_test']\n",
    "Y = df['label']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Building the model\n",
    "\n",
    "pipe = Pipeline([('vectorizer', bow_vector)\n",
    "                 ,('classifier', rfc)])\n",
    "\n",
    "pipe.fit(X_train, Y_train)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Random Forest Accuracy:\\n\",metrics.accuracy_score(Y_test, predicted),\"\\n\") # Accuracy\n",
    "print(\"Random Forest Precision:\\n\",metrics.precision_score(Y_test, predicted, average='macro'),\"\\n\") # Precision\n",
    "print(\"Random Forest Recall:\\n\",metrics.recall_score(Y_test, predicted, average='macro'),\"\\n\") # Recall\n",
    "print(\"Random Forest F1 Score:\\n\",metrics.f1_score(Y_test, predicted, average='macro')) # F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold\n",
    "\n",
    "<center><span style=\"background-color:red; color:white; font-size:30px\">DO NOT RUN BELOW: this way is very slow, taking at least half an hour to process!!</span></center>\n",
    "\n",
    "`In order for this to work, we need to use the full data set not the reduced set.`\n",
    "\n",
    "Firstly, the testing text (df['body_test']) needs to be transformed and input into the vectoriser (Bag-of-Words or TF-IDF) so that it can be input into the Cross Validation Score object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform = bow_vector.fit_transform(df['body_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><span style=\"background-color:chartreuse\">This way does work but, as show by the big red signs, it is terribly slow! This is due to the data being uploaded to the spaCy model, which slows everything down. The LinkedIn example (shown later) has a better way which we will use later.</span></center>\n",
    "\n",
    "<center><span style=\"background-color:red; color:white; font-size:30px\">DO NOT RUN ABOVE: this way is very slow, taking at least half an hour to process!!</span></center>\n",
    "\n",
    "`The Random Forest classifier will be generated again for this part.`<br>\n",
    "With the Random Forest classifier, the K-Fold object will be generated next with the amount of folds, *K*, stated.\n",
    "\n",
    "Lastly, generate the 'Cross Validation Score' object to put all the components together to return a score. The cross_val_score object needs to know:\n",
    "\n",
    "- The model we're using = generate new one for this part\n",
    "- Input features = **X_features**\n",
    "- The label, i.e. the answer of the text = **df['label']** or **Y**\n",
    "- How we're splitting the data (cv) = **k_fold**\n",
    "- What scoring metric we use (scoring) =  in this case **accuracy**, another metric can be chosen\n",
    "- Best to put **n_jobs = -1** as each can be run independent of one another\n",
    "\n",
    "The output will be an array of showing the accuracy for each iteration of the model. In this case, **5** results should be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9       , 0.9       , 0.93      , 0.87      , 0.92929293])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "# 5 splits\n",
    "\n",
    "cross_val_score(estimator=rfc, X=X_transform, y=df['label'], cv=k_fold, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedIn Example = much quicker\n",
    "\n",
    "As mentioned, the text_cleaner intially developed, before showing the LinkedIn example, uses a spaCy model. These have their advantages as they return tokenised objects which have methods that can be used directly, e.g. Lemmatisation (*lemma_*) or Stop Words (*is_stop*). On the other hand, NLTK returns string objects that have been affected by NLTK classes, like a Lemmatiser.<br>\n",
    "**However**, the spaCy model can slow down the model as the model is applied to each string that comes from the file.\n",
    "\n",
    "The LinkedIn example will be quickly shown and then a basic code has been written below as not all of the stuff in the LinkedIn example is necessary for the K-Fold analysis.\n",
    "\n",
    "<hr>\n",
    "<center><span style=\"color:white; background-color: blue; font-size:20px\">Refresher on Lambdas</span></center>\n",
    "\n",
    "https://www.w3schools.com/python/python_lambda.asp\n",
    "\n",
    "\"*A lambda function is a small anonymous function; it can take any number of arguments, but can only have one expression.*\"\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just a: 15 \n",
      "\n",
      "a and b: 30\n"
     ]
    }
   ],
   "source": [
    "x = lambda a : a + 10\n",
    "\n",
    "print(\"Just a:\",x(5),\"\\n\")\n",
    "\n",
    "x = lambda a, b : a * b\n",
    "\n",
    "print(\"a and b:\",x(5,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda is better shown when you use them as an anonymous function inside another function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(n):\n",
    "    \n",
    "    return lambda a : a * n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use that function in many different ways, even using the same function to make 2 different functions in the same programme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Input: 10\n",
      "\n",
      "Double Function: 20 \n",
      "\n",
      "Triple Function: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Input: 10\\n\")\n",
    "\n",
    "myDoubler = myfunc(2)\n",
    "\n",
    "print(\"Double Function:\", myDoubler(10), \"\\n\")\n",
    "\n",
    "myTripler = myfunc(3)\n",
    "\n",
    "print(\"Triple Function:\", myTripler(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Below is how the **LinkedIn Learning** example used the for transforming the 'body_test' data, and then Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8094</th>\n",
       "      <th>8095</th>\n",
       "      <th>8096</th>\n",
       "      <th>8097</th>\n",
       "      <th>8098</th>\n",
       "      <th>8099</th>\n",
       "      <th>8100</th>\n",
       "      <th>8101</th>\n",
       "      <th>8102</th>\n",
       "      <th>8103</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%    0    1    2    3    4    5    6    7  ...  8094  8095  \\\n",
       "0       128     4.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1        49     4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2        62     3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3        28     7.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4       135     4.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "   8096  8097  8098  8099  8100  8101  8102  8103  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8106 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "ps = nltk.PorterStemmer()\n",
    "# Removes affixes from words - dies -> die - flies -> fli\n",
    "    # Similar to spacy's lemmatiser but ma not return actual words (as above)\n",
    "\n",
    "data = pd.read_csv(\"Files/SMSSpamCollection.tsv\", sep='\\t')\n",
    "# Reading the file\n",
    "\n",
    "data.columns = ['label', 'body_text']\n",
    "# Labeling the columns\n",
    "\n",
    "\n",
    "# Not sure this is necessary for the model\n",
    "def count_punct(text):\n",
    "\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    # Calculates the number of punctuation symbols in the text\n",
    "    \n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "    # Returns a percentage of the text that is punctucation symbols (excluding spaces)\n",
    "    \n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "# Number of characters in the text without the spaces\n",
    "\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "# Using the 'count_punct' function on the text\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    # Removing any punctuation characters\n",
    "        # The use of 'word' is a bit misleading\n",
    "    \n",
    "    tokens = re.split('\\W+', text)\n",
    "    # Splitting the words using regex\n",
    "    \n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    # Using 'Porter Stemmer' to remove word affixes\n",
    "        # Not quite as good as Lemmatisation\n",
    "    \n",
    "    return text\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "\n",
    "X_features = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97935368, 0.97935368, 0.97843666, 0.96585804, 0.97484277])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "# 5 splits\n",
    "\n",
    "cross_val_score(estimator=rfc, X=X_features, y=data['label'], cv=k_fold, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X_features has a couple of bits of extra information that isn't needed for the K-Fold analysis of a Random Forest classifier.\n",
    "\n",
    "### Simplified Example\n",
    "\n",
    "Below is a simpler cleaning function, compared to the LinkedIn example, but is quicker (MUCH quicker) compared to the first code.<br>\n",
    "The code is split up to make things easier; firstly the set up similar to other classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>8796</th>\n",
       "      <th>8797</th>\n",
       "      <th>8798</th>\n",
       "      <th>8799</th>\n",
       "      <th>8800</th>\n",
       "      <th>8801</th>\n",
       "      <th>8802</th>\n",
       "      <th>8803</th>\n",
       "      <th>8804</th>\n",
       "      <th>8805</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8806 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  8796  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "3     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "4     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "   8797  8798  8799  8800  8801  8802  8803  8804  8805  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 8806 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import string\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import winsound\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Reading .csv file\n",
    "\n",
    "df = pd.read_csv(\"Files/SMSSpamCollection.tsv\", sep='\\t')\n",
    "\n",
    "df.columns = ['label','body_test']\n",
    "# File doesn't have column headers - now they have names assigned to their columns\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating custom tokeniser and cleaning function\n",
    "\n",
    "Lemmatiser = nltk.stem.WordNetLemmatizer()\n",
    "# Instantiating the NLTK Lemmatiser\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Putting punctuation symbols into an object\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Import spacy model\n",
    "\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# A list of stopwords that can be filtered out\n",
    "    # NLTK also has a stop words object but it has fewer words\n",
    "\n",
    "def text_cleaner(sentence):    \n",
    "                \n",
    "    sentence = \"\".join([char for char in sentence.strip() if char not in punctuations])\n",
    "    # Getting rid of any punctuation characters\n",
    "    \n",
    "    myTokens = re.split('\\W+', sentence)\n",
    "    # Tokenising the words\n",
    "    \n",
    "    myTokens = [token.lower() for token in myTokens if token not in stopwords]\n",
    "    # Removing stop words\n",
    "    \n",
    "    myTokens = [Lemmatiser.lemmatize(token) for token in myTokens]\n",
    "    # Lemmatising the words and putting in lower case except for proper nouns\n",
    "    \n",
    "    return myTokens    \n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating Vectoriser\n",
    "\n",
    "bow_vector = CountVectorizer(tokenizer = text_cleaner, ngram_range=(1,1))\n",
    "\n",
    "#tfidf_vector = TfidfVectorizer(tokenizer = text_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the LinkedIn code, we can show the results of the vectoriser. However, this is not necessary when developing the actual code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Transforming the data\n",
    "\n",
    "X_bow = bow_vector.fit_transform(df['body_test'])\n",
    "\n",
    "X_transform = pd.concat([pd.DataFrame(X_bow.toarray())], axis=1)\n",
    "\n",
    "winsound.PlaySound(\"Files/Alarm07.wav\", winsound.SND_FILENAME)\n",
    "\n",
    "X_transform.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard way (for myself anyway) is to use the Pipeline to vectorise and classify the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97935368 0.97576302 0.97484277 0.95867026 0.96585804]\n"
     ]
    }
   ],
   "source": [
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "    \n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "pipe = Pipeline([('vectorizer', bow_vector)\n",
    "                 ,('classifier', rfc)])\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "# 5 splits\n",
    "\n",
    "print(cross_val_score(estimator=pipe, X=df['body_test'], y=df['label'], cv=k_fold, scoring='accuracy', n_jobs=-1))\n",
    "\n",
    "winsound.PlaySound(\"Files/Alarm07.wav\", winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also use the vectorisation transformation so that the vectoriser isn't needed as it's already been used.<br>\n",
    "Either way, the same result is attained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With X_bow:\n",
      " [0.97845601 0.97666068 0.97663971 0.9640611  0.96495957]\n"
     ]
    }
   ],
   "source": [
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "    \n",
    "rfc = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "# 5 splits\n",
    "\n",
    "print(cross_val_score(estimator=rfc, X=X_bow, y=df['label'], cv=k_fold, scoring='accuracy', n_jobs=-1))\n",
    "\n",
    "winsound.PlaySound(\"Files/Alarm07.wav\", winsound.SND_FILENAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
