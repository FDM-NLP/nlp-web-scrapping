{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595583307405",
   "display_name": "Python 3.8.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1><center> Gradient Boosting </center></h1>\n",
    "\n",
    "*https://www.linkedin.com/learning/nlp-with-python-for-machine-learning-essential-training/introducing-gradient-boosting?u=78163626*\n",
    "\n",
    "*https://towardsdatascience.com/https-medium-com-talperetz24-mastering-the-new-generation-of-gradient-boosting-db04062a7ea2*\n",
    "\n",
    "*https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/*\n",
    "\n",
    "## *Written by Nathanael Hitch*\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CatBoost** and **Light GBM (LGBM)** are algorithms for gradient boosting decision trees, similar to **Random Forest**.\n",
    "\n",
    "# Gradient Boosting\n",
    "\n",
    "Gradient boosting is an *Ensemble Method*: it creates multiple models and then combines them to produce better results than a single model.\n",
    "\n",
    "Unlike other ensemble methods (e.g. Random Forest), Gradient Boosting takes an iterative approach to combining weak learners to create a strong learner by focusing on mistakes in prior iterations.\n",
    "\n",
    "The decision trees within Gradient Boosting are very basic, more basic than in other ensemble methods. For the first iteration/decision tree, it evaulates what it gets right and wrong. Then, with the next iteration, it places a heavier weight on what the first tree got wrong. It does this over and over, focusing on the examples that it doesn't quite understand until it has minimised the error as much as possible.\n",
    "\n",
    "Gradient Boosting models can accept various types of inputs, can be used for classification or regression and outputs feature importance.\n",
    "\n",
    "## Differences from other Ensemble Methods, e.g. Random Forest\n",
    "\n",
    "They are both models that use Ensemble Methods and Decision tress.\n",
    "\n",
    "However, Random Forest uses *bagging* while Gradient Boosting uses *boosting*; bagging samples randomly while boosting samples with an increased weight of ones it goes wrong previously.\n",
    "\n",
    "Random Forest can train in parallel as each decision tree doesn't rely on the previous. The training can be done reasonably quickly.<br>\n",
    "Gradient Boosting trains iteratively as relys on the trees before it. As trees can't be trained in parallel, the training for Gradient Boosting models is done much more slowly. This is a big consideration.\n",
    "\n",
    "Random Forest's final prediction is an unweighted voting while Gradient Boosting models have a weighted voting final prediciton.\n",
    "\n",
    "Lastly, Random Forest is **easier to tune**, **quicker to train** and **harder to overfit**; all positives.<br>\n",
    "Gradient Boosting is **harder to tune**, **slower to train** and **easier to overfit**; all negatives.\n",
    "\n",
    "It is harder to tune as it has more parameters and it is more likely to overfit as it *obsesses* over the ones it got wrong.\n",
    "\n",
    "So why go with Gradient Boosting; because, typically, these models are more powerful and perform better *when tuned properly*.\n",
    "\n",
    "These *boosts* usually rely on data that uses **Encoding**.\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Note\n",
    "\n",
    "Weirdly, while the examples in the ***NLP_Encoding.ipynb*** used the *Oridinal* or *One Hot* encoder on the primary data, the below example needed to use the *LabelEncoder* only, otherwise the metrics for *precision* and *recall* threw an error.\n",
    "\n",
    "Try running it afterwards without using the *LabelEncoder* on the labels, and/or *Oridinal* and *One Hot* encoders on the body data.\n",
    "\n",
    "When writing Gradient Boosting models, use all the different variations of encoders to see which one works.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Example\n",
    "\n",
    "Using the Gradient Booster, the Label Encoder for the *labels* column and using ***text_cleaner()*** function found in *NLP_Functions.py*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from NLP_Functions import text_cleaner\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Files/SMSSpamCollection.tsv\", sep='\\t')\n",
    "df.columns = ['label','body_test']\n",
    "\n",
    "encoder_lb = LabelEncoder()\n",
    "y_trans = encoder_lb.fit_transform(df['label'].astype(str))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['body_test'].astype(str), y_trans, test_size=0.2)\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = text_cleaner)\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=10)\n",
    "\n",
    "pipe = Pipeline([(\"vectorizer\", tfidf_vector),\n",
    "                (\"classifier\", gbc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Pipeline(steps=[('vectorizer',\n                 TfidfVectorizer(tokenizer=<function text_cleaner at 0x0000020624C15550>)),\n                ('classifier', GradientBoostingClassifier(n_estimators=10))])"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.9021543985637342\nPrecision: 1.0\nRecall: 0.1925925925925926\n"
    }
   ],
   "source": [
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search for Gradient Boosting\n",
    "\n",
    "As stated earlier, the parameters need to be tweaked in order for the Gradient Boosting model to work correctly. If not, the model's issues (easy to overfit etc.) can make the model worse than most others.\n",
    "\n",
    "To see the varying performance of Gradient Boosting for each parameters, we can set up a **Grid Search** to vary the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from NLP_Functions import text_cleaner\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pipeline(steps=[('vectorizer',\n                 TfidfVectorizer(tokenizer=<function text_cleaner at 0x0000020624C15550>)),\n                ('classifier', GradientBoostingClassifier())])\n"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Files/SMSSpamCollection.tsv\", sep='\\t')\n",
    "df.columns = ['label','body_test']\n",
    "\n",
    "encoder_lb = LabelEncoder()\n",
    "y_trans = encoder_lb.fit_transform(df['label'].astype(str))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['body_test'].astype(str), y_trans, test_size=0.2)\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = text_cleaner)\n",
    "\n",
    "gbc = GradientBoostingClassifier() #n_estimators=10)\n",
    "\n",
    "pipe = Pipeline([(\"vectorizer\", tfidf_vector),\n",
    "                (\"classifier\", gbc)])\n",
    "\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'classifier__n_estimators': [50, 100],\n",
    "                    'classifier__max_depth': [3, 7, 11, 15],\n",
    "                    'classifier__learning_rate': [0.01, 0.1, 1]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "GridSearchCV(estimator=Pipeline(steps=[('vectorizer',\n                                        TfidfVectorizer(tokenizer=<function text_cleaner at 0x0000020624C15550>)),\n                                       ('classifier',\n                                        GradientBoostingClassifier())]),\n             param_grid=[{'classifier__learning_rate': [0.01, 0.1, 1],\n                          'classifier__max_depth': [3, 7, 11, 15],\n                          'classifier__n_estimators': [50, 100]}],\n             scoring='accuracy')"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "clf = GridSearchCV(\n",
    "        pipe, param_grid=tuned_parameters, scoring = 'accuracy'\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Best parameters set found on development set:\n\n{'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__n_estimators': 100}\n\nGrid scores on development set:\n\n0.869 (+/-0.000) for {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 50}\n0.902 (+/-0.008) for {'classifier__learning_rate': 0.01, 'classifier__max_depth': 3, 'classifier__n_estimators': 100}\n0.874 (+/-0.005) for {'classifier__learning_rate': 0.01, 'classifier__max_depth': 7, 'classifier__n_estimators': 50}\n0.929 (+/-0.009) for {'classifier__learning_rate': 0.01, 'classifier__max_depth': 7, 'classifier__n_estimators': 100}\n0.869 (+/-0.001) for {'classifier__learning_rate': 0.01, 'classifier__max_depth': 11, 'classifier__n_estimators': 50}\n0.944 (+/-0.016) for {'classifier__learning_rate': 0.01, 'classifier__max_depth': 11, 'classifier__n_estimators': 100}\n0.868 (+/-0.002) for {'classifier__learning_rate': 0.01, 'classifier__max_depth': 15, 'classifier__n_estimators': 50}\n0.951 (+/-0.019) for {'classifier__learning_rate': 0.01, 'classifier__max_depth': 15, 'classifier__n_estimators': 100}\n0.945 (+/-0.011) for {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 50}\n0.956 (+/-0.016) for {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 100}\n0.960 (+/-0.018) for {'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__n_estimators': 50}\n0.967 (+/-0.017) for {'classifier__learning_rate': 0.1, 'classifier__max_depth': 7, 'classifier__n_estimators': 100}\n0.962 (+/-0.014) for {'classifier__learning_rate': 0.1, 'classifier__max_depth': 11, 'classifier__n_estimators': 50}\n0.966 (+/-0.014) for {'classifier__learning_rate': 0.1, 'classifier__max_depth': 11, 'classifier__n_estimators': 100}\n0.964 (+/-0.019) for {'classifier__learning_rate': 0.1, 'classifier__max_depth': 15, 'classifier__n_estimators': 50}\n0.965 (+/-0.017) for {'classifier__learning_rate': 0.1, 'classifier__max_depth': 15, 'classifier__n_estimators': 100}\n0.949 (+/-0.009) for {'classifier__learning_rate': 1, 'classifier__max_depth': 3, 'classifier__n_estimators': 50}\n0.949 (+/-0.006) for {'classifier__learning_rate': 1, 'classifier__max_depth': 3, 'classifier__n_estimators': 100}\n0.960 (+/-0.012) for {'classifier__learning_rate': 1, 'classifier__max_depth': 7, 'classifier__n_estimators': 50}\n0.960 (+/-0.007) for {'classifier__learning_rate': 1, 'classifier__max_depth': 7, 'classifier__n_estimators': 100}\n0.962 (+/-0.012) for {'classifier__learning_rate': 1, 'classifier__max_depth': 11, 'classifier__n_estimators': 50}\n0.961 (+/-0.018) for {'classifier__learning_rate': 1, 'classifier__max_depth': 11, 'classifier__n_estimators': 100}\n0.963 (+/-0.018) for {'classifier__learning_rate': 1, 'classifier__max_depth': 15, 'classifier__n_estimators': 50}\n0.964 (+/-0.012) for {'classifier__learning_rate': 1, 'classifier__max_depth': 15, 'classifier__n_estimators': 100}\n\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       0.97      1.00      0.98       956\n           1       0.98      0.78      0.87       158\n\n    accuracy                           0.97      1114\n   macro avg       0.97      0.89      0.93      1114\nweighted avg       0.97      0.97      0.97      1114\n\n--------------------------------------------------------------------------------\n"
    }
   ],
   "source": [
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "            % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print()\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print()\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"--\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest accuracy was 96.7% while the lowest accuracy was 86.8%; a difference of 10%.\n",
    "\n",
    "That is a massive difference in accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light GBM\n",
    "\n",
    "**Light Gradient Boosting Machine** (LGBM) is a library developed at Microsoft that provides an efficient implementation of the gradient boosting algorithm.\n",
    "\n",
    "The primary benefit of the LightGBM is the changes to the training algorithm that make the process dramatically faster, and in many cases, result in a more effective model.\n",
    "\n",
    "The LightGBM library provides wrapper classes so that the efficient algorithm implementation can be used with the scikit-learn library, specifically via:\n",
    "\n",
    "- LGBMClassifier\n",
    "- LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports for both the classifier and the regressor\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "The example below evaluates an LGBMClassifier on the test problem using repeated k-fold cross-validation and reports the mean accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.934 (0.021)\n"
    }
   ],
   "source": [
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "\n",
    "# evaluate the model\n",
    "model = LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the example *fits* a single model on all available data and a single prediction is made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Prediction: 1\n"
    }
   ],
   "source": [
    "# fit the model on the whole dataset\n",
    "model = LGBMClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# make a single prediction\n",
    "row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor\n",
    "\n",
    "The example below first evaluates an LGBMRegressor on the test problem using repeated k-fold cross-validation and reports the mean absolute error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import RepeatedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "MAE: -12.739 (1.408)\n"
    }
   ],
   "source": [
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
    "\n",
    "# evaluate the model\n",
    "model = LGBMRegressor()\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the example *fits* a single model on all available data and a single prediction is made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Prediction: -82.040\n"
    }
   ],
   "source": [
    "# fit the model on the whole dataset\n",
    "model = LGBMRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "# make a single prediction\n",
    "row = [[2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %.3f' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost\n",
    "\n",
    "**CatBoost** is a third-party library developed at Yandex that provides an efficient implementation of the gradient boosting algorithm.\n",
    "\n",
    "The primary benefit of the CatBoost (in addition to computational speed improvements) is support for categorical input variables. This gives the library its name CatBoost for *Category Gradient Boosting*.\n",
    "\n",
    "Like *LGBM*, the CatBoost library provides wrapper classes so that the efficient algorithm implementation can be used with the scikit-learn library, specifically via:\n",
    "\n",
    "- CatBoostClassifier\n",
    "- CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for both the classifier and the regressor\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "The example below evaluates an CatBoost Classifier on the test problem using repeated k-fold cross-validation and reports the mean accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.926 (0.026)\n"
    }
   ],
   "source": [
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "\n",
    "# evaluate the model\n",
    "model = CatBoostClassifier(verbose=0, n_estimators=100)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the example *fits* a single model on all available data and a single prediction is made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Prediction: 1\n"
    }
   ],
   "source": [
    "# fit the model on the whole dataset\n",
    "model = CatBoostClassifier(verbose=0, n_estimators=100)\n",
    "model.fit(X, y)\n",
    "\n",
    "# make a single prediction\n",
    "row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor\n",
    "\n",
    "The example below first evaluates an CatBoost Regressor on the test problem using repeated k-fold cross-validation and reports the mean absolute error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import RepeatedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "MAE: -9.623 (0.930)\n"
    }
   ],
   "source": [
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n",
    "\n",
    "# evaluate the model\n",
    "model = CatBoostRegressor(verbose=0, n_estimators=100)\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Prediction: -87.936\n"
    }
   ],
   "source": [
    "# fit the model on the whole dataset\n",
    "model = CatBoostRegressor(verbose=0, n_estimators=100)\n",
    "model.fit(X, y)\n",
    "\n",
    "# make a single prediction\n",
    "row = [[2.02220122, 0.31563495, 0.82797464, -0.30620401, 0.16003707, -1.44411381, 0.87616892, -0.50446586, 0.23009474, 0.76201118]]\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %.3f' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}