{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Logisitic Regression Classification </center></h1>\n",
    "\n",
    "*https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/*\n",
    "\n",
    "## *Written by Nathanael Hitch*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "***\n",
    "This model will look at the review text of Amazon products and predict whether it is positive or negative.\n",
    "***\n",
    "\n",
    "The information is a tab-separated-value file (.tsv) with 5 columns:\n",
    "\n",
    "- Rating: rating each user gave the Alexa (out of 5)\n",
    "- Date: date of the review\n",
    "- Variation: the model the user is reviewing\n",
    "- Verified_Review: text of each review\n",
    "- Feedback: contains a sentiment label; 1 = positive, 2 = negative\n",
    "\n",
    "The feedback column already includes whether the review was positive or negative, we can use that to test the model.\n",
    "\n",
    "Start with uploading the necessary data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating       date         variation  \\\n",
      "0       5  31-Jul-18  Charcoal Fabric    \n",
      "1       5  31-Jul-18  Charcoal Fabric    \n",
      "2       4  31-Jul-18    Walnut Finish    \n",
      "3       5  31-Jul-18  Charcoal Fabric    \n",
      "4       5  31-Jul-18  Charcoal Fabric    \n",
      "\n",
      "                                    verified_reviews  feedback  \n",
      "0                                      Love my Echo!         1  \n",
      "1                                          Loved it!         1  \n",
      "2  Sometimes while playing a game, you can answer...         1  \n",
      "3  I have had a lot of fun with this thing. My 4 ...         1  \n",
      "4                                              Music         1   \n",
      "\n",
      "(3150, 5) \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3150 entries, 0 to 3149\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   rating            3150 non-null   int64 \n",
      " 1   date              3150 non-null   object\n",
      " 2   variation         3150 non-null   object\n",
      " 3   verified_reviews  3150 non-null   object\n",
      " 4   feedback          3150 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 123.2+ KB\n",
      "None \n",
      "\n",
      "1    2893\n",
      "0     257\n",
      "Name: feedback, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Importing needed packages\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "df_amazon = pd.read_csv(\"Files/amazon_alexa.tsv\", sep=\"\\t\")\n",
    "# Loading the Amazon .tsv file.\n",
    "    # 'sep' is the delimeter to use - can't automatically detect the separator\n",
    "    # '\\t' = tab\n",
    "    \n",
    "\"\"\" Useful DataFrame (df) functions \"\"\"\n",
    "\n",
    "print(df_amazon.head(),\"\\n\")\n",
    "# The first 5 records from the DataFrame\n",
    "\n",
    "print(df_amazon.shape,\"\\n\")\n",
    "# Returns a tuple of the dimensions of the DataFrame\n",
    "\n",
    "print(df_amazon.info(),\"\\n\")\n",
    "# View data information\n",
    "\n",
    "print(df_amazon.feedback.value_counts())\n",
    "# Values of each feedback option (i.e. 1 or 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use spaCy to tokenise the data, strip information we don't need (stopwords, punctuation etc.), perform Lemmatisation and lowercaste the text.\n",
    "\n",
    "`Print's have been commented out for when creating the Logistic Regression Model at the end.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'sentence', 'test', 'test', 'London']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string # Contains a useful list of punctuation marks.\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Create list of punctuation marks\n",
    "\n",
    "def spacy_cleaner(sentence):\n",
    "    \n",
    "    #print(\"Input sentence:\\n\", sentence,\"\\n\")\n",
    "    \n",
    "    doc = nlp(sentence)\n",
    "    # Pass text into model's pipeline.\n",
    "    \n",
    "    myTokens = [token for token in doc]\n",
    "    # Creating a list of the words in the sentence.\n",
    "    #print(\"Sentence tokenised:\\n\", myTokens,\"\\n\")\n",
    "    \n",
    "    myTokens = [token for token in myTokens if token.is_stop == False and token.text not in punctuations]\n",
    "    # List of words without stopwords or punctuations.\n",
    "    #print(\"Sentence without stopwords or punctuations:\\n\", myTokens, \"\\n\")\n",
    "    \n",
    "    myTokens = [token.lemma_.strip().lower() if token.pos_ != \"PROPN\" else token.lemma_.strip() \\\n",
    "                for token in myTokens]\n",
    "    # Words are lemmatised, spaces at end removed and (if not a proper noun) lowercased.\n",
    "    \n",
    "    #print(\"Sentence lemmatisted, no spaces and lowercase (except Proper Noun):\\n\", myTokens, \"\\n\")\n",
    "    \n",
    "    return myTokens\n",
    "    \n",
    "spacy_cleaner(\"This is a test sentence, for testing tests from London.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further clean our text data we will create a **custom transformer**, removing end spaces and converting text into lower case. Here, we will create a custom predictors class wich inherits the TransformerMixin class(used to transform data/text). This class overrides the transform, fit and get_parrams methods. Weâ€™ll also create a clean_text() function that removes spaces and converts text into lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer using spaCy\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "class predictors(TransformerMixin):\n",
    "# Inheriting from TransformerMixin\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "    # fit: used for training your model without any pre-processing on the data    \n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify text in positive or negative labels is called Sentiment Analysis; to do that we need to represent our text numerically.<br>\n",
    "We can, amongst other ways, use the Bag-of-Words model to convert the text into a matrix of how many times a word has occured.\n",
    "\n",
    "We can generate a BoW matrix by using *Scikit-Learn*'s **CountVectoriser**. We will tell the CountVectoriser to use our function *spacy_cleaner* as its tokeniser, and define the n-gram range. For this one, we will be using uni-grams (one words), and the ngrams will be assigned to 'bow_vector'.<br>\n",
    "It would be good to look at the TF-IDF; this can be generated by using the *Scikit-Learn*'s **TfidfVectoriser**, with the tokeniser our *spacy_cleaner* function and the result assigned to 'tfidf_vector':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "#bow_vector = CountVectorizer(tokenizer = spacy_cleaner, ngram_range=(1,1))\n",
    "bow_vector = CountVectorizer(ngram_range=(1,1))\n",
    "# Bag-of-Words n-gram matrix\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_cleaner)\n",
    "# TF-IDF result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data between **'Training'** and **'Test'** sets\n",
    "\n",
    "We will use half the data set as a **training** set, and the other half as a **testing** set.<br>\n",
    "Fortunately, *scikit-learn* has a built in function for doing this, **train_test_split()**:\n",
    "\n",
    "- X = what we want to split\n",
    "- ylabels = labels we want to test aganist\n",
    "- Plus the size of the test set, in percentage form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Splitting between Training and Testing sets \"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df_amazon is the .tsv file that has been previously loaded:\n",
    "    # df_amazon = pd.read_csv(\"Files/amazon_alexa.tsv\", sep=\"\\t\")\n",
    "\n",
    "X = df_amazon['verified_reviews'] # 'verified_reviews' is what we want to analyse\n",
    "\n",
    "ylabels = df_amazon['feedback'] # 'feedback' is the label/answer to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)\n",
    "# Appling X, ylabels and the test size as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the **Logistice Regression Model**\n",
    "\n",
    "Start by importing the LogisiticRegression module; then create a LogisticRegression classifier object. \n",
    "\n",
    "*For this example*, we will build a pipeline with 3 components:\n",
    "\n",
    "- Classifier: clean and preprocess the text\n",
    "- Vectoriser: creates a BoW matrix for our text\n",
    "- Classifier: performs a logisitc regression to classify sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x000001248707A848>),\n",
       "                ('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Classifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "# Creating the LogisticRegression classifier object\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "    # These will using previously made function\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "pipe.fit(X_train,y_train)\n",
    "# Generating/training the model using the previously stated training sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Let's see how our model actually performs not that it has been trained.<br>\n",
    "This is done by using the *metrics* module; we will put our test data through the pipeline to come up with predictions and use the various functions from the metrics module to look at aspects of the pipeline:\n",
    "\n",
    "- Accuracy: percentage of the total predictions that are completely correct.\n",
    "- Precision: ratio of true positives plus false positive.\n",
    "- Recall: ratio of true positives to true positives plus false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9365079365079365\n",
      "Logistic Regression Precision: 0.9442586399108138\n",
      "Logistic Regression Recall: 0.9883313885647608\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted)) # Accuracy\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted)) # Precision\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted)) # Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the model correctly identified the comment's sentiment **94.1%** of the time.<br>\n",
    "When a review was predicted as positive, it was positive **95%** of the time<br>\n",
    "When given a positive review, the model identified it as positive **98.6%** of the time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
