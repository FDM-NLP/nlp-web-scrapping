{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmation that it works:\n",
      "\n",
      "Logistic Regression Accuracy:\n",
      " 0.9328478964401294\n",
      "Logistic Regression Precision:\n",
      " 0.9349269088399522\n",
      "Logistic Regression Recall:\n",
      " 0.9315204145922483\n"
     ]
    }
   ],
   "source": [
    "                        # Importing needed packages\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Reading .csv file\n",
    "    \n",
    "################################################################################################################\n",
    "                ##### Filepaths will need to be changed #####\n",
    "################################################################################################################\n",
    "\n",
    "df_train = pd.read_csv(\"Files/raw/tweets-train.csv\")\n",
    "    \n",
    "df_train = pd.read_csv(\"Files/raw/tweets-test.csv\")\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating custom cleaner function\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Create list of punctuation marks\n",
    "\n",
    "def spacy_cleaner(sentence):\n",
    "    \n",
    "    #print(\"Input sentence:\\n\", sentence,\"\\n\")\n",
    "    \n",
    "    doc = nlp(sentence.strip())\n",
    "    # Pass text into model's pipeline.\n",
    "    \n",
    "    myTokens = [token for token in doc]\n",
    "    # Creating a list of the words in the sentence.\n",
    "    #print(\"Sentence tokenised:\\n\", myTokens,\"\\n\")\n",
    "    \n",
    "    myTokens = [token for token in myTokens if token.is_stop == False and token.text not in punctuations]\n",
    "    # List of words without stopwords or punctuations.\n",
    "    #print(\"Sentence without stopwords or punctuations:\\n\", myTokens, \"\\n\")\n",
    "    \n",
    "    myTokens = [token.lemma_.strip().lower() if token.pos_ != \"PROPN\" else token.lemma_.strip() \\\n",
    "                for token in myTokens]\n",
    "    # Words are lemmatised, spaces at end removed and (if not a proper noun) lowercased.\n",
    "    \n",
    "    myTokens = [token for token in myTokens if token != \"\"]\n",
    "    \n",
    "    #print(\"Sentence lemmatisted, no spaces and lowercase (except Proper Noun):\\n\", myTokens, \"\\n\")\n",
    "    \n",
    "    return myTokens\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating Bag-of-Words Vectoriser\n",
    "\n",
    "bow_vector = CountVectorizer(tokenizer = spacy_cleaner, ngram_range=(1,1))\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Splitting into Training and Testing sets\n",
    "\n",
    "X_train = df_train['text']\n",
    "Y_train = df_train['sentiment']\n",
    "\n",
    "X_test = df_test['text'] # 'text' is what we want to analyse\n",
    "Y_test = df_test['sentiment'] # 'sentiment' is the label/answer to test against\n",
    "\n",
    "# Below needed if splitting one .csv file into training and testing sets\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Building OVR Logisitic Regression Classifier\n",
    "    \n",
    "ovr = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "pipe = Pipeline([('vectorizer', bow_vector)\n",
    "                 ,('classifier', ovr)])\n",
    "\n",
    "pipe.fit(X_train, Y_train)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Confirmation that it works:\\n\")\n",
    "print(\"Logistic Regression Accuracy:\\n\",metrics.accuracy_score(Y_test, predicted)) # Accuracy\n",
    "print(\"Logistic Regression Precision:\\n\",metrics.precision_score(Y_test, predicted, average='macro')) # Precision\n",
    "print(\"Logistic Regression Recall:\\n\",metrics.recall_score(Y_test, predicted, average='macro')) # Recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
